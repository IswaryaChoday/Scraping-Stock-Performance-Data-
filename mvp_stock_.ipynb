{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: beautifulsoup4 in /Users/dinesh.paladhi/anaconda2/envs/py36/lib/python3.6/site-packages (4.6.0)\n",
      "Requirement already satisfied: requests in /Users/dinesh.paladhi/anaconda2/envs/py36/lib/python3.6/site-packages (2.18.4)\n",
      "Requirement already satisfied: urllib3<1.23,>=1.21.1 in /Users/dinesh.paladhi/anaconda2/envs/py36/lib/python3.6/site-packages (from requests) (1.22)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/dinesh.paladhi/anaconda2/envs/py36/lib/python3.6/site-packages (from requests) (2018.4.16)\n",
      "Requirement already satisfied: idna<2.7,>=2.5 in /Users/dinesh.paladhi/anaconda2/envs/py36/lib/python3.6/site-packages (from requests) (2.6)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /Users/dinesh.paladhi/anaconda2/envs/py36/lib/python3.6/site-packages (from requests) (3.0.4)\n",
      "Requirement already satisfied: pandas in /Users/dinesh.paladhi/anaconda2/envs/py36/lib/python3.6/site-packages (0.22.0)\n",
      "Requirement already satisfied: python-dateutil>=2 in /Users/dinesh.paladhi/anaconda2/envs/py36/lib/python3.6/site-packages (from pandas) (2.7.2)\n",
      "Requirement already satisfied: pytz>=2011k in /Users/dinesh.paladhi/anaconda2/envs/py36/lib/python3.6/site-packages (from pandas) (2018.4)\n",
      "Requirement already satisfied: numpy>=1.9.0 in /Users/dinesh.paladhi/anaconda2/envs/py36/lib/python3.6/site-packages (from pandas) (1.14.2)\n",
      "Requirement already satisfied: six>=1.5 in /Users/dinesh.paladhi/anaconda2/envs/py36/lib/python3.6/site-packages (from python-dateutil>=2->pandas) (1.11.0)\n",
      "Requirement already satisfied: mysql-connector-python in /Users/dinesh.paladhi/anaconda2/envs/py36/lib/python3.6/site-packages (8.0.21)\n",
      "Requirement already satisfied: protobuf>=3.0.0 in /Users/dinesh.paladhi/anaconda2/envs/py36/lib/python3.6/site-packages (from mysql-connector-python) (3.12.2)\n",
      "Requirement already satisfied: setuptools in /Users/dinesh.paladhi/anaconda2/envs/py36/lib/python3.6/site-packages (from protobuf>=3.0.0->mysql-connector-python) (39.1.0)\n",
      "Requirement already satisfied: six>=1.9 in /Users/dinesh.paladhi/anaconda2/envs/py36/lib/python3.6/site-packages (from protobuf>=3.0.0->mysql-connector-python) (1.11.0)\n"
     ]
    }
   ],
   "source": [
    "! pip install beautifulsoup4\n",
    "! pip install requests\n",
    "! pip install pandas\n",
    "! pip install mysql-connector-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import mysql.connector\n",
    "from bs4 import BeautifulSoup\n",
    "import uuid \n",
    "import time\n",
    "import datetime\n",
    "from dateutil.parser import parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "songplay_table_drop = \"DROP TABLE IF EXISTS songplays;\"\n",
    "user_table_drop = \"DROP TABLE IF EXISTS users;\"\n",
    "song_table_drop = \"DROP TABLE IF EXISTS songs;\"\n",
    "artist_table_drop = \"DROP TABLE IF EXISTS artists;\"\n",
    "time_table_drop = \"DROP TABLE IF EXISTS time;\"\n",
    "\n",
    "# CREATE TABLES\n",
    "\n",
    "songplay_table_create = (\"\"\"create table if not exists songplays(songplay_id SERIAL PRIMARY KEY NOT NULL, start_time timestamp,user_id int,level varchar, song_id varchar, artist_id varchar,session_id int,location varchar,user_agent varchar);\"\"\")\n",
    "\n",
    "user_table_create = (\"\"\"create table if not exists users(user_id int PRIMARY KEY NOT NULL,first_name varchar,last_name varchar,gender varchar, level varchar);\"\"\")\n",
    "\n",
    "song_table_create = (\"\"\"create table if not exists songs(song_id varchar PRIMARY KEY NOT NULL,title varchar,artist_id varchar,year int,duration numeric);\"\"\")\n",
    "\n",
    "artist_table_create = (\"\"\"create table if not exists artists(artist_id varchar PRIMARY KEY NOT NULL, name varchar, location varchar, latitude numeric, longitude numeric);\"\"\")\n",
    "\n",
    "time_table_create = (\"\"\"create table if not exists time(start_time timestamp PRIMARY KEY NOT NULL, hour int, day int, week int, month int, year int, weekday int);\"\"\")\n",
    "\n",
    "\n",
    "create_table_queries = [songplay_table_create, user_table_create, song_table_create, artist_table_create, time_table_create]\n",
    "drop_table_queries = [songplay_table_drop, user_table_drop, song_table_drop, artist_table_drop, time_table_drop]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mysql.connector\n",
    "\n",
    "\n",
    "def create_database():\n",
    "    conn = mysql.connector.connect(\n",
    "      host=\"localhost\",\n",
    "      user=\"root\",\n",
    "      password=\"\"\n",
    "    )\n",
    "    cur = conn.cursor()\n",
    "\n",
    "    cur.execute(\"CREATE DATABASE stock_performance_1\")\n",
    "    \n",
    "    return cur, conn\n",
    "\n",
    "def drop_tables(cur, conn):\n",
    "    for query in drop_table_queries:\n",
    "        cur.execute(query)\n",
    "        conn.commit()\n",
    "\n",
    "\n",
    "def create_tables(cur, conn):\n",
    "    for query in create_table_queries:\n",
    "        cur.execute(query)\n",
    "        conn.commit()\n",
    "\n",
    "\n",
    "def main():\n",
    "    cur, conn = create_database()\n",
    "    print(\"Created database \")\n",
    "    \n",
    "    drop_tables(cur, conn)\n",
    "    create_tables(cur, conn)\n",
    "\n",
    "    conn.close()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "main_url = 'https://hamptonroadsalliance.com/'\n",
    "try:\n",
    "    page = requests.get(main_url, timeout=5)\n",
    "    sector_soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    main_page_content = sector_soup.find(id = 'page#26')\n",
    "except:\n",
    "    print(\"Could not scrape for some reason\")\n",
    "\n",
    "# Get Sector Names\n",
    "sectors_list = ()\n",
    "for a in main_page_content.find_all('a', href=True): \n",
    "    if a.text: \n",
    "        value = a['href']\n",
    "        sectors_list=sectors_list+(value.replace('/',''),)\n",
    "\n",
    "# Get Sectors list\n",
    "sectors_list_array = []\n",
    "for value in sectors_list:\n",
    "    sectors_list_array.append((str(uuid.uuid4()), value))\n",
    "print(sectors_list_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Database Connection\n",
    "\n",
    "mydb = mysql.connector.connect(\n",
    "    host=\"localhost\",\n",
    "    user=\"root\",\n",
    "    password=\"\",\n",
    "    database=\"stock_performance\"\n",
    ")\n",
    "mycursor = mydb.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Insert data into sector table   \n",
    "sql = \"INSERT INTO sector (sector_id, sector_name) VALUES (%s, %s)\"\n",
    "mycursor.executemany(sql,sectors_list_array)\n",
    "mydb.commit()\n",
    "print(mycursor.rowcount, \"record inserted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get companies list as tuple\n",
    "for sector in sectors_list:\n",
    "    try:\n",
    "        page = requests.get(main_url+sector,timeout=5)\n",
    "        company_soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    except:\n",
    "        print(\"Could not scrape for some reason\")\n",
    "    companies_list = ()\n",
    "    companies_list = companies_list + ()\n",
    "    for a in company_soup.find_all(\"h4\",\"el-title uk-h4 uk-margin-top uk-margin-remove-bottom\"):\n",
    "        if a.text:\n",
    "            value = a.text.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get companies_list_array as group of companies\n",
    "# Get sector_companies_list_array\n",
    "import uuid\n",
    "sector_companies_list =()\n",
    "sector_companies_list_array = []\n",
    "companies_list = ()\n",
    "companies_list_array = []\n",
    "for sector in sectors_list:\n",
    "    sql = \"SELECT sector_id FROM sector where sector_name=%s\"\n",
    "    adr = (sector,)\n",
    "    mycursor.execute(sql, adr)\n",
    "    sector_id = mycursor.fetchone()[0]\n",
    "    try:\n",
    "        page = requests.get(main_url+sector, timeout=5)\n",
    "        soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    except:\n",
    "        print(\"Could not scrape for some reason\")\n",
    "    sector_companies_list = ()\n",
    "    sector_companies_list = sector_companies_list + (sector_id,)\n",
    "    for a in soup.find_all(\"h4\",\"el-title uk-h4 uk-margin-top uk-margin-remove-bottom\"):\n",
    "        if a.text:\n",
    "            value = a.text.strip()\n",
    "            uuid_value = str(uuid.uuid4())\n",
    "            companies_list=companies_list+(uuid_value,value)+ ('',)\n",
    "            sector_companies_list = sector_companies_list + (uuid_value,)\n",
    "            sector_companies_list_array.append(sector_companies_list)\n",
    "            sector_companies_list = ()\n",
    "            sector_companies_list = sector_companies_list + (sector_id,)\n",
    "            companies_list_array.append(companies_list)\n",
    "            companies_list = ()\n",
    "\n",
    "print (len(companies_list_array))\n",
    "print (len(sector_companies_list_array))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert into company table \n",
    "insert_into_company_table = \"INSERT INTO company (id, name, stock_symbol) VALUES (%s, %s, %s)\"\n",
    "mycursor.executemany(insert_into_company_table,companies_list_array)\n",
    "mydb.commit()\n",
    "print(mycursor.rowcount, \"record inserted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert into and company_sector table\n",
    "insert_into_sector_company_table = \"INSERT INTO company_sector (sector_id, company_id) VALUES (%s, %s)\"\n",
    "mycursor.executemany(insert_into_sector_company_table,sector_companies_list_array) \n",
    "mydb.commit()\n",
    "print(mycursor.rowcount, \"record inserted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "mydb = mysql.connector.connect(\n",
    "    host=\"localhost\",\n",
    "    user=\"root\",\n",
    "    password=\"\",\n",
    "    database=\"stock_performance\"\n",
    ")\n",
    "mycursor = mydb.cursor()\n",
    "\n",
    "# Get companies with not null stock_symbol\n",
    "\n",
    "sql = \"SELECT id,stock_symbol FROM company where stock_symbol is not null\"\n",
    "mycursor.execute(sql)\n",
    "company_data = mycursor.fetchall()\n",
    "\n",
    "company_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Construct data for inserting into stock_price_history table\n",
    "import uuid\n",
    "\n",
    "sql = \"SELECT id FROM stock_source where source_name='Yahoo Finance'\"\n",
    "mycursor.execute(sql)\n",
    "price_source_id = mycursor.fetchone()[0]\n",
    "print(price_source_id)\n",
    "\n",
    "\n",
    "stock_history_array = []\n",
    "stock_history = ()\n",
    "for i in company_data:\n",
    "    ## Get Market\n",
    "    try:\n",
    "        page = requests.get('https://finance.yahoo.com/quote/'+i[1], timeout=5)\n",
    "        stock_market_soup = BeautifulSoup(page.text, 'html.parser')\n",
    "        market = stock_market_soup.find('div',{'id':'quote-header-info'}).find('span').text\n",
    "    except:\n",
    "        print(\"Could not scrape for some reason\")\n",
    "    print(i[1],market)\n",
    "    time.sleep(1)\n",
    "    \n",
    "    ## Get Stock_history \n",
    "    try:\n",
    "        page = requests.get('https://finance.yahoo.com/quote/'+i[1]+'/history', timeout=5)\n",
    "        stock_history_soup = BeautifulSoup(page.text, 'html.parser')\n",
    "    except:\n",
    "        print(\"Could not scrape for some reason\")\n",
    "    count = 0\n",
    "#     stock_history = (uuid_value,i[0],i[1],market,)\n",
    "    for item in stock_history_soup.find('table',{'data-test':'historical-prices'}).find('tbody'):\n",
    "        uuid_value = str(uuid.uuid4())\n",
    "        stock_history = (uuid_value,i[0],i[1],market,price_source_id,)\n",
    "        if(count < 10):\n",
    "            count = count + 1\n",
    "            for value in item.find_all('span'):\n",
    "                stock_history = stock_history + (value.text,)\n",
    "            stock_history_array.append(stock_history)\n",
    "    time.sleep(1)\n",
    "# print(stock_history_array)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## Data cleaning to inserting empty string if volume doesn't exist\n",
    "for i in range(len(stock_history_array)):\n",
    "    print(len(stock_history_array[i]))\n",
    "    if(len(stock_history_array[i]) < 11):\n",
    "        stock_history_array[i] = stock_history_array[i] +('',)\n",
    "    \n",
    "## Format date correctly and remove ',' from volume\n",
    "for i in range(len(stock_history_array)):\n",
    "    stock_history_array_list = list(stock_history_array[i])\n",
    "    stock_history_array_list[4] = datetime.datetime.strptime(stock_history_array_list[4], '%b %d, %Y')\n",
    "    stock_history_array_list[10] = stock_history_array_list[10].replace(\",\", \"\")\n",
    "    stock_history_array[i] = tuple(stock_history_array_list)\n",
    "print(stock_history_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Insert into stock_price_history table\n",
    "\n",
    "insert_into_stock_price_history_table = \"INSERT INTO stock_price_history (id, company_id, stock_symbol, market,price_source_id, Date, open_price, high_price, low_price, close_price, adj_close_price, volume) VALUES (%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s)\"\n",
    "mycursor.executemany(insert_into_stock_price_history_table,stock_history_array) \n",
    "mydb.commit()\n",
    "print(mycursor.rowcount, \"record inserted\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Seeking Alpha - Get Stock Market News Data\n",
    "import uuid\n",
    "import time\n",
    "\n",
    "# Get Scraping source id \n",
    "\n",
    "sql = \"SELECT id FROM stock_source where source_name='Seeking Alpha Feeds'\"\n",
    "mycursor.execute(sql)\n",
    "news_source_id = mycursor.fetchone()[0]\n",
    "print(news_source_id)\n",
    "\n",
    "## Scrape data to insert into stock_news table\n",
    "\n",
    "seeking_alpha_news_items_list = []\n",
    "for i in company_data:\n",
    "    try:\n",
    "        sa_stock_market_news_page = requests.get('https://seekingalpha.com/api/sa/combined/'+i[1]+'.xml', timeout=5)\n",
    "        sa_stock_market_news_soup = BeautifulSoup(sa_stock_market_news_page.content,features=\"xml\")\n",
    "        sa_items = sa_stock_market_news_soup.findAll('item')\n",
    "    except:\n",
    "        print(\"Could not scrape for some reason\")\n",
    "    for item in sa_items:\n",
    "        seeking_alpha_news_items = ()\n",
    "        date = parse(item.pubDate.text).date()\n",
    "        uuid_value = str(uuid.uuid4())\n",
    "        seeking_alpha_news_items = seeking_alpha_news_items + (uuid_value,i[0],i[1],item.title.text,item.link.text,date,item.author_name.text,news_source_id,)\n",
    "        seeking_alpha_news_items_list.append(seeking_alpha_news_items)\n",
    "        print(seeking_alpha_news_items)\n",
    "    time.sleep(1)\n",
    "print(len(seeking_alpha_news_items_list))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Insert into stock_news table\n",
    "\n",
    "insert_into_stock_news_table = \"INSERT INTO stock_news (id, company_id, stock_symbol, news_title, news_url, published_date, author, news_source_id ) VALUES (%s,%s,%s,%s,%s,%s,%s,%s)\"\n",
    "mycursor.executemany(insert_into_stock_news_table,seeking_alpha_news_items_list) \n",
    "mydb.commit()\n",
    "print(mycursor.rowcount, \"record inserted\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "import time\n",
    "## Get news from Yahoo Finance \n",
    "\n",
    "# Get Scraping source id \n",
    "\n",
    "sql = \"SELECT id FROM stock_source where source_name='Yahoo Finance'\"\n",
    "mycursor.execute(sql)\n",
    "news_source_id = mycursor.fetchone()[0]\n",
    "# print(news_source_id)\n",
    "\n",
    "yahoo_finance_news_item_list = []\n",
    "for i in company_data:\n",
    "    list_of_news_urls = []\n",
    "    yahoo_news_page = requests.get('https://finance.yahoo.com/quote/'+i[1],timeout=5)\n",
    "    yahoo_news_soup = BeautifulSoup(yahoo_news_page.text, 'html.parser')\n",
    "    yahoo_news_data = yahoo_news_soup.find_all('div',{'id':'quoteNewsStream-0-Stream'})\n",
    "    print(\"New Company\")\n",
    "    # Get only 5 News urls data per one company \n",
    "    count = 0\n",
    "    for item in yahoo_news_data:\n",
    "        a_tags = item.find_all('a')\n",
    "        for a_tag_item in a_tags:\n",
    "            yahoo_finance_news_item = ()\n",
    "            uuid_value = str(uuid.uuid4())\n",
    "            if a_tag_item.has_attr('href'):\n",
    "                count= count+1\n",
    "                news_url = 'https://finance.yahoo.com' + a_tag_item.attrs['href']\n",
    "                \n",
    "                ## Scrape single news page for published_date, title and author information\n",
    "                try:\n",
    "                    yahoo_single_news_page = requests.get(news_url,timeout=5)\n",
    "                    yahoo_single_news_page.raise_for_status()\n",
    "                    yahoo_single_news_soup = BeautifulSoup(yahoo_single_news_page.text, 'html.parser')\n",
    "                    yahoo_single_news_data = yahoo_single_news_soup.find('div',{'id':'YDC-Side-StackCompositeSideTop'})\n",
    "                    news_title = yahoo_single_news_data.find('h1',{'class':'Lh(36px) Fz(25px)--sm Fz(32px) Mb(17px)--sm Mb(20px) Mb(30px)--lg Ff($ff-primary) Lts($lspacing-md) Fw($fweight) Fsm($fsmoothing) Fsmw($fsmoothing) Fsmm($fsmoothing) Wow(bw)'}).text\n",
    "                    author = yahoo_single_news_data.find('div',{'class':'auth-prov-soc Mend(4px) Va(m) D(tbc) Mah(45px) Mah(40px)--sm Maw(320px) Fz(14px)'}).find('a').text\n",
    "                    published_date = parse(yahoo_single_news_data.find('time').text).date()\n",
    "                except:\n",
    "                    print(\"Some problem\")\n",
    "                \n",
    "                # Creating a single news record\n",
    "                yahoo_finance_news_item = yahoo_finance_news_item + (uuid_value,i[0],i[1],news_title,news_url,published_date,author,news_source_id,)\n",
    "                # Adding each record into a list \n",
    "                yahoo_finance_news_item_list.append(yahoo_finance_news_item)\n",
    "                print(yahoo_finance_news_item)\n",
    "                if(count==5):\n",
    "                    break\n",
    "            time.sleep(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Insert into stock_news table\n",
    "\n",
    "insert_into_stock_news_table = \"INSERT INTO stock_news (id, company_id, stock_symbol, news_title, news_url, published_date, author, news_source_id ) VALUES (%s,%s,%s,%s,%s,%s,%s,%s)\"\n",
    "mycursor.executemany(insert_into_stock_news_table,yahoo_finance_news_item_list) \n",
    "mydb.commit()\n",
    "print(mycursor.rowcount, \"record inserted\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
